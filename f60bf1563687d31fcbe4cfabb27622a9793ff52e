{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "ebfcb7eb_859203a8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1019989
      },
      "writtenOn": "2024-03-28T12:08:28Z",
      "side": 1,
      "message": "After using this patch, I found some strange thread stack information during the test, as many as hundreds. When vdbench stops, these thread locks will be executed.\n\nAlso, the performance will drop a lot, I\u0027m not sure if it has something to do with this.\n\n#0  0x00007fa3e460d575 in pthread_rwlock_wrlock () from /lib64/libpthread.so.0\n#1  0x00007fa3e8571f80 in mdcache_getattrs (obj_hdl\u003d0x9eba038, attrs_out\u003d0x7fa3e19fdcd0) //\tPTHREAD_RWLOCK_wrlock(\u0026entry-\u003eattr_lock);\n#2  0x00007fa3e854b9c1 in nfs_SetPreOpAttr (obj\u003d0x9eba038, attr\u003d0x7fa3e19fdf40) \n#3  0x00007fa3e855e599 in nfs3_write (arg\u003d0x6e5be50, req\u003d0x6e5b600, res\u003d0x36ceb180) \n#4  0x00007fa3e846338f in nfs_rpc_process_request (reqdata\u003d0x6e5b600, retry\u003dfalse) \n#5  0x00007fa3e84639a9 in nfs_rpc_valid_NFS (req\u003d0x6e5b600) \n#6  0x00007fa3e8030fba in svc_vc_decode () from /lib64/libntirpc.so.5.0\n#7  0x00007fa3e802bd3a in svc_request () from /lib64/libntirpc.so.5.0\n#8  0x00007fa3e8030eba in svc_vc_recv () from /lib64/libntirpc.so.5.0\n#9  0x00007fa3e802bcba in svc_rqst_xprt_task_recv () from /lib64/libntirpc.so.5.0\n#10 0x00007fa3e803a0f7 in work_pool_thread () from /lib64/libntirpc.so.5.0\n#11 0x00007fa3e46082ff in start_thread () from /lib64/libpthread.so.0\n#12 0x00007fa3e3639dd3 in clone () from /lib64/libc.so.6",
      "revId": "f60bf1563687d31fcbe4cfabb27622a9793ff52e",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "233d56cb_d174f8c3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1019989
      },
      "writtenOn": "2024-03-28T13:01:04Z",
      "side": 1,
      "message": "Another similar stack is:\n#0  0x00007fa3e460d575 in pthread_rwlock_wrlock () from /lib64/libpthread.so.0\n#1  0x00007fa3e8571f80 in mdcache_getattrs (obj_hdl\u003d0x9fc6538, attrs_out\u003d0x7fa3e15fdc50) \n#2  0x00007fa3e854b7cd in nfs_SetPostOpAttr (obj\u003d0x9fc6538, Fattr\u003d0x3843aca8, attrs\u003d0x0) \n#3  0x00007fa3e854bab5 in nfs_SetWccData (before_attr\u003d0x0, obj\u003d0x9fc6538, wcc_data\u003d0x3843ac88) \n#4  0x00007fa3e855dfc5 in nfs3_complete_write (data\u003d0xc1fff10) \n#5  0x00007fa3e855ea4a in nfs3_write (arg\u003d0xb5fa350, req\u003d0xb5f9b00, res\u003d0x3843ac80) \n#6  0x00007fa3e846338f in nfs_rpc_process_request (reqdata\u003d0xb5f9b00, retry\u003dfalse) \n#7  0x00007fa3e84639a9 in nfs_rpc_valid_NFS (req\u003d0xb5f9b00)",
      "parentUuid": "ebfcb7eb_859203a8",
      "revId": "f60bf1563687d31fcbe4cfabb27622a9793ff52e",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "36743288_780f8d4c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1003571
      },
      "writtenOn": "2024-03-28T21:38:07Z",
      "side": 1,
      "message": "Hmm, that\u0027s a RW lock not a mutex, I wonder how that has any relation to this?",
      "parentUuid": "233d56cb_d174f8c3",
      "revId": "f60bf1563687d31fcbe4cfabb27622a9793ff52e",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "530885e4_8eb08709",
        "filename": "src/include/fsal_types.h",
        "patchSetId": 1
      },
      "lineNbr": 981,
      "author": {
        "id": 1021665
      },
      "writtenOn": "2024-03-28T07:21:49Z",
      "side": 1,
      "message": "Wondering if there would be a case, in which we would need to have a separate mutex as well for the IO work? because we now signal fd cond, then broadcast io cond and then we unlock the same mutex...",
      "revId": "f60bf1563687d31fcbe4cfabb27622a9793ff52e",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "de548d3d_4442e453",
        "filename": "src/include/fsal_types.h",
        "patchSetId": 1
      },
      "lineNbr": 981,
      "author": {
        "id": 1003571
      },
      "writtenOn": "2024-03-28T21:38:07Z",
      "side": 1,
      "message": "I don\u0027t think so. Whenever a thread uses pthread_cond_signal or pthread_cond_broadcast, it is OK to drop the mutex. And adding a second mutex would mean start I/O would need to take it also, which would introduce lock ordering constraints that might actually make it so you always have to take both...\n\nNow one issue is the thread that signals and broadcasts both condition variables CAN then block between signalling fd_work and broadcasting io_work if an fd work thread wakes up and takes the mutex. It would be neat if we had more complex mechanisms that would allow us to drop the mutex, schedule all the waiting threads (1 or more fd_work, all the io_work) and then re-take the mutex. And then it would be handy if there was a signal and release function so a signalling thread wouldn\u0027t have to wait for a woken up thread to do work - I wonder how much that\u0027s actually an issue, does the scheduler take signal/broadcast as a yield point?",
      "parentUuid": "530885e4_8eb08709",
      "revId": "f60bf1563687d31fcbe4cfabb27622a9793ff52e",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    }
  ]
}