diff a/src/FSAL/Stackable_FSALs/FSAL_MDCACHE/mdcache_lru.c b/src/FSAL/Stackable_FSALs/FSAL_MDCACHE/mdcache_lru.c	(rejected hunks)
@@ -395,40 +385,45 @@ cond_pin_entry(cache_entry_t *entry, uint32_t flags)
  * @param[in] entry  The entry to clean
  */
 static inline void
-cache_inode_lru_clean(cache_entry_t *entry)
+mdcache_lru_clean(mdcache_entry_t *entry)
 {
-	cache_inode_status_t cache_status = CACHE_INODE_SUCCESS;
-
-	if (is_open(entry)) {
-		cache_status =
-		    cache_inode_close(entry,
-				      CACHE_INODE_FLAG_REALLYCLOSE |
-				      CACHE_INODE_FLAG_NOT_PINNED |
-				      CACHE_INODE_FLAG_CLEANUP |
-				      CACHE_INODE_DONT_KILL);
-		if (cache_status != CACHE_INODE_SUCCESS) {
+	fsal_status_t status;
+
+	if (fsal_is_open(&entry->obj_handle)) {
+		status = fsal_close(&entry->obj_handle);
+		if (FSAL_IS_ERROR(status)) {
			LogCrit(COMPONENT_CACHE_INODE_LRU,
-				"Error closing file in cleanup: %d.",
-				cache_status);
+				"Error closing file in cleanup: %s.",
+				fsal_err_txt(status));
		}
	}

-	if (entry->type == DIRECTORY)
-		cache_inode_release_dirents(entry, CACHE_INODE_AVL_BOTH);
+	if (entry->obj_handle.type == DIRECTORY) {
+		status = mdcache_dirent_invalidate_all(entry);
+		if (FSAL_IS_ERROR(status)) {
+			LogCrit(COMPONENT_CACHE_INODE_LRU,
+				"Error releasing dirents in cleanup: %s.",
+				fsal_err_txt(status));
+		}
+	}

-	/* Free FSAL resources */
-	if (entry->obj_handle) {
-		entry->obj_handle->obj_ops.release(entry->obj_handle);
-		entry->obj_handle = NULL;
+	/* Free SubFSAL resources */
+	if (entry->sub_handle) {
+		subcall(
+			entry->sub_handle->obj_ops.release(entry->sub_handle)
+		       );
+		entry->sub_handle = NULL;
	}

+	/* Clean our handle */
+	fsal_obj_handle_fini(&entry->obj_handle);
+
	/* Clean out the export mapping before deconstruction */
-	clean_mapping(entry);
+	mdc_clean_mapping(entry);

	/* Finalize last bits of the cache entry */
-	cache_inode_key_delete(&entry->fh_hk.key);
+	mdcache_key_delete(&entry->fh_hk.key);
	PTHREAD_RWLOCK_destroy(&entry->content_lock);
-	PTHREAD_RWLOCK_destroy(&entry->state_lock);
	PTHREAD_RWLOCK_destroy(&entry->attr_lock);
 }

@@ -619,6 +628,100 @@ void cache_inode_lru_cleanup_try_push(cache_entry_t *entry)
	}
 }

+static int
+run_lane(size_t lane, uint64_t *totalclosed)
+{
+	/* The amount of work done on this lane on this pass. */
+	size_t workdone = 0;
+	/* The entry being examined */
+	mdcache_lru_t *lru = NULL;
+	struct lru_q *q;
+	/* Number of entries closed in this run. */
+	size_t closed = 0;
+	fsal_status_t status;
+	/* a cache entry */
+	mdcache_entry_t *entry;
+	/* Current queue lane */
+	struct lru_q_lane *qlane = &LRU[lane];
+	/* entry refcnt */
+	uint32_t refcnt;
+
+	q = &qlane->L1;
+
+	LogDebug(COMPONENT_CACHE_INODE_LRU,
+		 "Reaping up to %d entries from lane %zd",
+		 lru_state.per_lane_work, lane);
+
+	QLOCK(qlane);
+	qlane->iter.active = true;	/* ACTIVE */
+	/* While for_each_safe per se is NOT MT-safe, the iteration can be made
+	 * so by the convention that any competing thread which would invalidate
+	 * the iteration also adjusts glist and (in particular) glistn */
+	glist_for_each_safe(qlane->iter.glist, qlane->iter.glistn, &q->q) {
+		struct lru_q *q;
+
+		/* check per-lane work */
+		if (workdone >= lru_state.per_lane_work)
+			goto next_lane;
+
+		lru = glist_entry(qlane->iter.glist, mdcache_lru_t, q);
+		/* XXX dang open-coded ref... */
+		refcnt = atomic_inc_int32_t(&lru->refcnt);
+
+		/* get entry early */
+		entry = container_of(lru, mdcache_entry_t, lru);
+
+		/* check refcnt in range */
+		if (unlikely(refcnt > 2)) {
+			/* XXX dang but calls unref function... */
+			mdcache_lru_unref(entry, LRU_UNREF_QLOCKED);
+			workdone++; /* but count it */
+			/* qlane LOCKED, lru refcnt is
+			 * restored */
+			continue;
+		}
+
+		/* Move entry to MRU of L2 */
+		q = &qlane->L1;
+		LRU_DQ_SAFE(lru, q);
+		q = &qlane->L2;
+		lru_insert(lru, q, LRU_LRU);
+		++(q->size);
+
+		/* Drop the lane lock while performing (slow) operations on
+		 * entry */
+		QUNLOCK(qlane);
+
+		/* Acquire the content lock first; we may need to look at fds
+		 * and close it. */
+		PTHREAD_RWLOCK_wrlock(&entry->content_lock);
+		if (fsal_is_open(&entry->obj_handle)) {
+			status = fsal_close(&entry->obj_handle);
+			if (FSAL_IS_ERROR(status)) {
+				LogCrit(COMPONENT_CACHE_INODE_LRU,
+					"Error closing file in LRU thread.");
+			} else {
+				++(*totalclosed);
+				++closed;
+			}
+		}
+		PTHREAD_RWLOCK_unlock(&entry->content_lock);
+
+		QLOCK(qlane);	/* QLOCKED */
+		/* XXX dang here too */
+		mdcache_lru_unref(entry, LRU_UNREF_QLOCKED);
+		++workdone;
+	} /* for_each_safe lru */
+
+ next_lane:
+	qlane->iter.active = false; /* !ACTIVE */
+	QUNLOCK(qlane);
+	LogDebug(COMPONENT_CACHE_INODE_LRU,
+		 "Actually processed %zd entries on lane %zd closing %zd descriptors",
+		 workdone, lane, closed);
+	return workdone;
+}
+
 /**
  * @brief Function that executes in the lru thread
  *
@@ -669,12 +772,6 @@ void cache_inode_lru_cleanup_try_push(cache_entry_t *entry)
  * @param[in] ctx Fridge context
  */

-#define CL_FLAGS \
-	(CACHE_INODE_FLAG_REALLYCLOSE| \
-	 CACHE_INODE_FLAG_NOT_PINNED| \
-	 CACHE_INODE_FLAG_CONTENT_HAVE| \
-	 CACHE_INODE_FLAG_CONTENT_HOLD)
-
 static void
 lru_run(struct fridgethr_context *ctx)
 {
@@ -695,14 +792,13 @@ lru_run(struct fridgethr_context *ctx)
	uint64_t totalclosed = 0;
	/* The current count (after reaping) of open FDs */
	size_t currentopen = 0;
-	struct lru_q *q;
	time_t new_thread_wait;

	SetNameFunction("cache_lru");

	fds_avg = (lru_state.fds_hiwat - lru_state.fds_lowat) / 2;

-	if (cache_param.use_fd_cache)
+	if (mdcache_param.use_fd_cache)
		extremis = (atomic_fetch_size_t(&open_fd_count) >
			    lru_state.fds_hiwat);

@@ -763,119 +861,12 @@ lru_run(struct fridgethr_context *ctx)
		do {
			workpass = 0;
			for (lane = 0; lane < LRU_N_Q_LANES; ++lane) {
-				/* The amount of work done on this lane on
-				   this pass. */
-				size_t workdone = 0;
-				/* The entry being examined */
-				cache_inode_lru_t *lru = NULL;
-				/* Number of entries closed in this run. */
-				size_t closed = 0;
-				/* a cache_status */
-				cache_inode_status_t cache_status =
-				    CACHE_INODE_SUCCESS;
-				/* a cache entry */
-				cache_entry_t *entry;
-				/* Current queue lane */
-				struct lru_q_lane *qlane = &LRU[lane];
-				/* entry refcnt */
-				uint32_t refcnt;
-
-				q = &qlane->L1;
-
-				LogDebug(COMPONENT_CACHE_INODE_LRU,
-					 "Reaping up to %d entries from lane %zd",
-					 lru_state.per_lane_work, lane);
-
				LogFullDebug(COMPONENT_CACHE_INODE_LRU,
-					     "formeropen=%zd totalwork=%zd workpass=%zd closed:%zd totalclosed:%"
-					     PRIu64,
-					     formeropen, totalwork, workpass,
-					     closed, totalclosed);
-
-				QLOCK(qlane);
-				qlane->iter.active = true;	/* ACTIVE */
-				/* While for_each_safe per se is NOT MT-safe,
-				 * the iteration can be made so by the
-				 * convention that any competing thread which
-				 * would invalidate the iteration also adjusts
-				 * glist and (in particular) glistn */
-				glist_for_each_safe(qlane->iter.glist,
-						    qlane->iter.glistn, &q->q) {
-					struct lru_q *q;
-
-					/* check per-lane work */
-					if (workdone >= lru_state.per_lane_work)
-						goto next_lane;
-
-					lru =
-					    glist_entry(qlane->iter.glist,
-							cache_inode_lru_t, q);
-					refcnt =
-					    atomic_inc_int32_t(&lru->refcnt);
-
-					/* get entry early */
-					entry =
-					    container_of(lru, cache_entry_t,
-							 lru);
-
-					/* check refcnt in range */
-					if (unlikely(refcnt > 2)) {
-						cache_inode_lru_unref(
-							entry,
-							LRU_UNREF_QLOCKED);
-						workdone++; /* but count it */
-						/* qlane LOCKED, lru refcnt is
-						 * restored */
-						continue;
-					}
-
-					/* Move entry to MRU of L2 */
-					q = &qlane->L1;
-					LRU_DQ_SAFE(lru, q);
-					lru->qid = LRU_ENTRY_L2;
-					q = &qlane->L2;
-					glist_add(&q->q, &lru->q);
-					++(q->size);
-
-					/* Drop the lane lock while performing
-					 * (slow) operations on entry */
-					QUNLOCK(qlane);
-
-					/* Acquire the content lock first; we
-					 * may need to look at fds and close
-					 * it. */
-					PTHREAD_RWLOCK_wrlock(&entry->
-							      content_lock);
-					if (is_open(entry)) {
-						cache_status =
-						    cache_inode_close(
-							    entry, CL_FLAGS);
-						if (cache_status !=
-						    CACHE_INODE_SUCCESS) {
-							LogCrit(
-						      COMPONENT_CACHE_INODE_LRU,
-							"Error closing file in LRU thread.");
-						} else {
-							++totalclosed;
-							++closed;
-						}
-					}
-					PTHREAD_RWLOCK_unlock(&entry->
-							      content_lock);
-
-					QLOCK(qlane);	/* QLOCKED */
-					cache_inode_lru_unref(
-						entry,
-						LRU_UNREF_QLOCKED);
-					++workdone;
-				} /* for_each_safe lru */
+					     "formeropen=%zd totalwork=%zd workpass=%zd totalclosed:%"
+					     PRIu64, formeropen, totalwork,
+					     workpass, totalclosed);

- next_lane:
-				qlane->iter.active = false; /* !ACTIVE */
-				QUNLOCK(qlane);
-				LogDebug(COMPONENT_CACHE_INODE_LRU,
-					 "Actually processed %zd entries on lane %zd closing %zd descriptors",
-					 workdone, lane, closed);
+				size_t workdone = run_lane(lane, &totalclosed);
				workpass += workdone;
			}	/* foreach lane */
			totalwork += workpass;
