{
  "comments": [
    {
      "key": {
        "uuid": "387368c1_1e412eaa",
        "filename": "src/FSAL/FSAL_CEPH/handle.c",
        "patchSetId": 2
      },
      "lineNbr": 2655,
      "author": {
        "id": 1004087
      },
      "writtenOn": "2020-12-14T19:26:28Z",
      "side": 1,
      "message": "When is this filled in?  I\u0027m not seeing it.  The export_id should be in the op_ctx at this point, so why can\u0027t we just fill it in now?",
      "revId": "a01c4a283f4c5395984570e59c6200c579e0a838",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "93bbc084_088930e8",
        "filename": "src/FSAL/FSAL_CEPH/handle.c",
        "patchSetId": 2
      },
      "lineNbr": 2655,
      "author": {
        "id": 1003571
      },
      "writtenOn": "2020-12-14T20:19:51Z",
      "side": 1,
      "message": "Hmm, actually, why is export_id even in the handle key? That can result in two mdcache entries for the same file if the same Ceph filesystem is exported multiple times.\n\nI see it set in wire_to_host and construct_handle",
      "parentUuid": "387368c1_1e412eaa",
      "revId": "a01c4a283f4c5395984570e59c6200c579e0a838",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2760b391_9e9a784a",
        "filename": "src/FSAL/FSAL_CEPH/handle.c",
        "patchSetId": 2
      },
      "lineNbr": 2655,
      "author": {
        "id": 1008909
      },
      "writtenOn": "2020-12-14T20:22:52Z",
      "side": 1,
      "message": "See commit e45743b4735ac4 (FSAL_CEPH: track export id as part of the host handle)",
      "parentUuid": "93bbc084_088930e8",
      "revId": "a01c4a283f4c5395984570e59c6200c579e0a838",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "87d84dd3_77b56231",
        "filename": "src/FSAL/FSAL_CEPH/handle.c",
        "patchSetId": 2
      },
      "lineNbr": 2655,
      "author": {
        "id": 1003571
      },
      "writtenOn": "2020-12-14T20:29:13Z",
      "side": 1,
      "message": "Oh, grumble... ok, I\u0027m a little bit remembering some conversation about this. So there is an issue in doing so that at least state conflicts may not be visible, especially for state not reflected into the filesystem (maybe just share reservations). There might also be issues with directories (since there would be two separate instances of the directory cache).\n\nLong term we may need to find a different solution for the cephx credentials issue...",
      "parentUuid": "2760b391_9e9a784a",
      "revId": "a01c4a283f4c5395984570e59c6200c579e0a838",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "80c81220_d5eee34d",
        "filename": "src/FSAL/FSAL_CEPH/handle.c",
        "patchSetId": 2
      },
      "lineNbr": 2655,
      "author": {
        "id": 1018367
      },
      "writtenOn": "2020-12-15T08:21:17Z",
      "side": 1,
      "message": "Actually, I confuse with fill export id timing. (Is there any chance that the op_ctx contains different export id once we export successfully?)\nI trace the Jeff\u0027s commits, looks like we want to fill export id by wire_to_host (that would take wire-handle opaque and translate to host-handle).\n\nAnd handle_to_wire does the things like the inverse of wire_to_host.\nThat would describe the full opaque. I am not sure should we fill export id in this moment.\n\nIn this patch set, I just want to keep the original behavior means does not fill export id when handle_to_wire. Calling wire_to_host and construct_handle would set up export id.\n\nAfter I trace again, I found that will check several times about export id before calling wire_to_host. I think maybe we could also fill export id when handle_to_wire? It should not have different between op_ctx and opaque.\n\nBTW, I afraid that I am misunderstanding with cephx cred issues, this means different export id with own cache would cause the above issues like frank says?",
      "parentUuid": "87d84dd3_77b56231",
      "revId": "a01c4a283f4c5395984570e59c6200c579e0a838",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}